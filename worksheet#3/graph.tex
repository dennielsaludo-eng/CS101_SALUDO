% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={graph},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{graph}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

title: ``Group Activity - Solar and Stellar Astrophysics'' author: ``MGA
KUPAL'' date: ``2025-12-01'' output: pdf\_document: latex\_engine:
xelatex html\_document: default ---

\{r setup, include=FALSE\} knitr::opts\_chunk\$set(echo = TRUE, warning
= FALSE, message = FALSE)

\subsubsection{Loading the library}\label{loading-the-library}

\{r\} library(rvest) library(dplyr) library(stringr)\\
library(lubridate) \#\# For date formats library(ggplot2)

\subsubsection{1. Creating an object}\label{creating-an-object}

\{r\} titles \textless- character(0) authors \textless- character(0)
submission\_dates \textless- character(0) originally\_announced
\textless- character(0) doi \textless- character(0)

\subsubsection{2. Importing the url and created a
structure}\label{importing-the-url-and-created-a-structure}

\{r, cache = TRUE\} \# Base URL for Solar and Stellar Astrophysics
(astro-ph.SR) base\_url \textless-
``\url{https://arxiv.org/search/?query=astro-ph.SR&searchtype=all&source=header&start=}''

all\_papers \textless- list()

\section{Loop 4 times to get 200 papers (0, 50, 100,
150)}\label{loop-4-times-to-get-200-papers-0-50-100-150}

starts \textless- seq(from = 0, to = 150, by = 50)

for (i in starts) \{

\# Construct URL url \textless- paste0(base\_url, i)
print(paste(``Scraping:'', url)) \# Print progress so you know it's
working

\# STANDARD SCRAPING (Replaces polite::scrape) \# We use tryCatch to
skip a page if an error occurs, preventing a total crash tryCatch(\{
page \textless- read\_html(url)

\begin{verbatim}
# Extract containers
papers_html <- page %>% html_nodes("li.arxiv-result")

# Extract Data Elements
titles <- papers_html %>% 
  html_node("p.title.is-5.mathjax") %>% 
  html_text(trim = TRUE)

authors <- papers_html %>% 
  html_node("p.authors") %>% 
  html_text(trim = TRUE) %>% 
  str_remove("Authors:\n")

abstracts <- papers_html %>% 
  html_node("span.abstract-full") %>% 
  html_text(trim = TRUE) %>% 
  str_remove("â–½ Less")

meta_raw <- papers_html %>% 
  html_node("p.is-size-7") %>% 
  html_text(trim = TRUE)

# Store in temporary dataframe
temp_df <- data.frame(
  title = titles,
  author = authors,
  abstract = abstracts,
  meta_raw = meta_raw,
  stringsAsFactors = FALSE
)

all_papers[[length(all_papers) + 1]] <- temp_df
\end{verbatim}

\}, error = function(e) \{ print(paste(``Error on page starting at'',
i)) \})

\# IMPORTANT: Wait 3 seconds between pages to avoid being banned by
arXiv Sys.sleep(3) \}

\section{Combine all lists into one
dataframe}\label{combine-all-lists-into-one-dataframe}

df\_papers \textless- bind\_rows(all\_papers)

\section{Check count}\label{check-count}

print(paste(``Total papers extracted:'', nrow(df\_papers)))

\subsubsection{3. Cleaning the data}\label{cleaning-the-data}

\{r\} df\_clean \textless- df\_papers \%\textgreater\% mutate( \# 1.
Extract Submission Date submission\_date\_text = str\_extract(meta\_raw,
``Submitted.*?(=?;)``), submission\_date\_text =
str\_remove\_all(submission\_date\_text,''Submitted \textbar;``),
submission\_date = dmy(submission\_date\_text),

\begin{verbatim}
# 2. Extract DOI
doi = str_extract(meta_raw, "doi:.*"),
doi = str_remove(doi, "doi:"),

# 3. Extract Announced Date (Backup if submission is missing)
announced_date_text = str_extract(meta_raw, "originally announced [A-Za-z]+ [0-9]{4}"),
announced_date_text = str_remove(announced_date_text, "originally announced "),
originally_announced = my(announced_date_text)
\end{verbatim}

)

\section{Remove rows where date might have failed
(optional)}\label{remove-rows-where-date-might-have-failed-optional}

df\_clean \textless- df\_clean \%\textgreater\%
filter(!is.na(submission\_date))

head(df\_clean \%\textgreater\% select(title, submission\_date, doi))

\subsubsection{4. Arranging the dates}\label{arranging-the-dates}

\{r\} df\_sorted \textless- df\_clean \%\textgreater\%
arrange(submission\_date)

\section{Display summary statistics}\label{display-summary-statistics}

print(paste(``Date range:'',
min(df\_sorted\(submission_date), "to", max(df_sorted\)submission\_date)))
print(paste(``Total papers after cleaning:'', nrow(df\_sorted)))

\subsubsection{5. Turning into a plot time
series}\label{turning-into-a-plot-time-series}

\{r\} \# Count papers per month papers\_per\_month \textless- df\_sorted
\%\textgreater\% mutate(month\_year = floor\_date(submission\_date,
``month'')) \%\textgreater\% group\_by(month\_year) \%\textgreater\%
summarise(count = n())

\section{Plot}\label{plot}

ggplot(papers\_per\_month, aes(x = month\_year, y = count)) +
geom\_line(color = ``darkblue'', size = 1) + geom\_point(color =
``red'') + labs(title = ``Time Series: Solar and Stellar Astrophysics
Papers (arXiv)'', subtitle = ``Frequency of papers submitted per month
(astro-ph.SR)'', x = ``Date'', y = ``Number of Papers'') +
theme\_minimal()

\subsubsection{6. Export to CSV
(Optional)}\label{export-to-csv-optional}

\{r\} \# Export the cleaned data write.csv(df\_sorted,
``arxiv\_astro-ph-SR\_papers.csv'', row.names = FALSE) print(``Data
exported to arxiv\_astro-ph-SR\_papers.csv'')

\end{document}
